{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "43008364-f300-47a8-9291-e56570861556",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import random\n",
    "from collections import namedtuple\n",
    "from itertools import count\n",
    "from PIL import Image\n",
    "\n",
    "\n",
    "import gym\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as T\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4920ff07-bb22-40c6-b7ce-5d347cc6d992",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up matplotlib\n",
    "is_ipython = 'inline' in matplotlib.get_backend()\n",
    "\n",
    "if is_ipython:\n",
    "    from IPython import display\n",
    "    \n",
    "plt.ion()\n",
    "\n",
    "# if gpu is to be used\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "93878522-315f-4506-9d70-ece1e8e33feb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e6a64ab5-3450-4790-9b35-125001c610b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('MountainCar-v0')\n",
    "env.reset()\n",
    "for _ in range(1000):\n",
    "    env.render()\n",
    "    env.step(env.action_space.sample())\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "27aee09f-c543-4be2-a9cb-8bc8bc1995cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getHeight(x_position):\n",
    "    return np.sin(3*x_position)*.45 + .55\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9e1f222d-7103-4b4b-88d2-0b7b3df1780f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def newreward(pos):\n",
    "    if (pos>=0.5):\n",
    "        return 2\n",
    "    else:\n",
    "        return (pos + 1.2)/1.8 - 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29301206-f387-4475-a11c-e859ec692d41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Q-learning function\n",
    "def QLearn(env, learning, epsilon, min_eps, episodes):\n",
    "    \"\"\"\n",
    "    Determine size of discretizeed state space.\n",
    "    \"\"\"\n",
    "    num_states = (env.observation_space.high - env.observation_space.low)*np.array([10, 50])\n",
    "    num_states = np.around(num_states, 0).astype(int) + 1\n",
    "    \n",
    "    # initialize Q table\n",
    "    Q = np.random.uniform(\n",
    "        low=-1,\n",
    "        high=0,\n",
    "        size=(num_states[0], num_states[1],\n",
    "             env.action_space.n)\n",
    "    )\n",
    "    \n",
    "    Qinit = np.copy(0)\n",
    "    \n",
    "    # Initiañize variables to track rewards\n",
    "    reward_list = []\n",
    "    eve-reward_list = []\n",
    "    \n",
    "    # Make copy of epsilon\n",
    "    eps1 = epsilon\n",
    "    \n",
    "    # Keep track of firs succes\n",
    "    first = episodes + 1\n",
    "    \n",
    "    # Run Q-learning algorithm\n",
    "    for i in range(episodes):\n",
    "        # Initiañize parameters\n",
    "        done = False\n",
    "        tot_reward, reward = 0, 0\n",
    "        state = env.reset()\n",
    "        \n",
    "        # Discretize state\n",
    "        state_adj = (state - env.observation_space.low)*np,array([10, 50])\n",
    "        state_adj = np.round(state_adj, 0).astype(int)\n",
    "        \n",
    "        while done!= True:\n",
    "            # Render environment for las few episodes\n",
    "            if i>=(episodes-5) or i<5:\n",
    "                env.render()\n",
    "            # Determine next action - epsilon greedy estrategy\n",
    "            \n",
    "            if np.random.random() < (1 - epsilon):\n",
    "                action = np.argmax(Q[state_adj[0], state_adj[0]])\n",
    "            else:\n",
    "                action = np.random.randint(0, env.action_space.n)\n",
    "                \n",
    "            # Get next state an reward\n",
    "            state2, reward, done, info = env.step(action)\n",
    "            \n",
    "            # Discretize state2\n",
    "            state2_adj = (state2 - env.observation-space.low)*np.array([10, 50])\n",
    "            state2_adj = np.round(state2_adj, 0).astype(int)\n",
    "            \n",
    "            # Save to Qpoints\n",
    "            row = np.array([state_adj[0], state_adj[1], action])                \n",
    "            \n",
    "            # Allow for terminal states\n",
    "            if done and state2[0]>=.5:\n",
    "                Q[state_adj[0], state_adj[0], action] += delta\n",
    "            \n",
    "            # Notifies of any clears\n",
    "            if (state2[0]>=.5) and (i<first):\n",
    "                first = 1\n",
    "                print(\"First clear on episode {}\".format(i+1))\n",
    "            # Minut 4.36\n",
    "            "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
